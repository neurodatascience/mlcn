{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q3\n",
    "## Q3.1\n",
    "We saw regularization using L1 and L2 losses. A third commonly-used penalty is ElasticNet, which is a weighted combination of the two. For the squared error, the metric would look like:\n",
    "\\begin{equation}\n",
    "    Err(Y, \\hat{Y}) = \\sum (Y-\\hat{Y})^2 + \\alpha ( \\beta L_1 + (1-\\beta) L_2 )\n",
    "\\end{equation}  \n",
    "Here, $\\alpha$ behaves as it did before: it balances the focus between the prediction error and the regularization term. The additional parameter, $\\beta$ balances the relative weight between the L1 and L2 losses. $\\beta$ values of 1 and 0 are the same as L1 and L2 regularization, respectively.  \n",
    "  \n",
    "For the noisy quadratic data that we saw in Section 3, include `sklearn`'s ElasticNet regression model in the comparison and examine the behaviour of your model with different values of $\\alpha$ and $\\beta$.  \n",
    "Between L1, L2, and ElasticNet, which one performs best? (you may need to increase the range of x or the amount of noise to see an appreciable difference).  \n",
    "\n",
    "Note: `sklearn` uses `l1_ratio` as the parameter name for $\\beta$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "x_data = np.random.random((200,1))\n",
    "y_label = (x_data-0.2)*(x_data-0.6) + np.random.randn(x_data.shape[0], 1)*0.10\n",
    "x_curve = np.linspace(0,1,200)\n",
    "y_curve = (x_curve-0.2)*(x_curve-0.6)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_label, train_size=0.4)\n",
    "plt.plot(x_data, y_label,'.')\n",
    "plt.plot(x_curve, y_curve, '.-')\n",
    "\n",
    "poly_feat = PolynomialFeatures(degree=4)\n",
    "feat_train = poly_feat.fit_transform(x_train)\n",
    "feat_test = poly_feat.fit_transform(x_test)\n",
    "\n",
    "elastic_test_scr_list = []\n",
    "elastic_train_scr_list = []\n",
    "elastic_coef_mag_list = []  # Let's look at the regularization error\n",
    "\n",
    "ridge_test_scr_list = []\n",
    "ridge_train_scr_list = []\n",
    "ridge_coef_mag_list = []\n",
    "\n",
    "lass_test_scr_list = []\n",
    "lass_train_scr_list = []\n",
    "lass_coef_mag_list = []\n",
    "\n",
    "for i in range(20):  ############ You can modify this value here\n",
    "    poly_feat = PolynomialFeatures(degree=i)\n",
    "    feat_train = poly_feat.fit_transform(x_train)\n",
    "    \n",
    "    # Elastic\n",
    "    elastic_mdl = ElasticNet(fit_intercept=False, l1_ratio=0.5, alpha=0.001)\n",
    "    elastic_mdl.fit(feat_train, y_train)\n",
    "    \n",
    "    # Ridge\n",
    "    ridge_mdl = Ridge(fit_intercept=False, alpha=0.1)     \n",
    "    ridge_mdl.fit(feat_train, y_train)\n",
    "    \n",
    "    # Lasso\n",
    "    lass_mdl = Lasso(fit_intercept=False, alpha=0.001)\n",
    "    lass_mdl.fit(feat_train, y_train)\n",
    "    \n",
    "    \n",
    "    # Evaluation\n",
    "    feat_test = poly_feat.fit_transform(x_test)\n",
    "    \n",
    "    elastic_train_scr_list.append(elastic_mdl.score(feat_train, y_train))\n",
    "    elastic_test_scr_list.append(elastic_mdl.score(feat_test, y_test))\n",
    "    elastic_coef_mag_list.append(elastic_mdl.coef_ @ elastic_mdl.coef_.T)\n",
    "    \n",
    "    ridge_train_scr_list.append(ridge_mdl.score(feat_train, y_train))\n",
    "    ridge_test_scr_list.append(ridge_mdl.score(feat_test, y_test))\n",
    "    ridge_coef_mag_list.append(np.squeeze(ridge_mdl.coef_ @ ridge_mdl.coef_.T))\n",
    "\n",
    "    lass_train_scr_list.append(lass_mdl.score(feat_train, y_train))\n",
    "    lass_test_scr_list.append(lass_mdl.score(feat_test, y_test))\n",
    "    lass_coef_mag_list.append(np.squeeze(lass_mdl.coef_ @ lass_mdl.coef_.T))\n",
    "    \n",
    "plt.figure()\n",
    "plt.plot(elastic_test_scr_list)\n",
    "plt.plot(ridge_test_scr_list)\n",
    "plt.plot(lass_test_scr_list)\n",
    "plt.legend(['Elastic','Ridge','Lasso'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performance seems to be Ridge, but we would need to perform cross validation across values of `alpha` and `l1_ratio` to verify."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3.2\n",
    "This question requires using a Python dictionary. A brief explanation is provided.  \n",
    "  \n",
    "Fit the following data using one of the regularized linear regression models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "x = (np.random.rand(500,1)-0.5)*5\n",
    "y = (x-3)*(x-0.1)*(x+2)*(4*x+4) + 15*np.random.randn(*x.shape)\n",
    "plt.plot(x,y,'.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can (should) use `sklearn.model_selection.GridSearchCV` to get the optimal value of your regularization weight.  \n",
    "The function expects a dictionary for the parameter, `parameter_grid`. Python recognizes `{`braces`}` as declaring a dictionary. Values are stored in a dictionary using a key. For example, `my_dict = {'chicken': 4}` would store the data `4` under the key `chicken` in the `my_dict` dictionary. You can get to the stored data using brackets, like you would access an array with indices:  \n",
    "`my_dict['chicken']`  \n",
    "  \n",
    "The dictionary you need to define must use the name of the parameter you're optimizing as a key, followed by a list of values across which you would like to optimize. For example, if you supplied:  \n",
    "`my_param_dict = {'alpha': [0.1, 0.5, 1, 10]}`  \n",
    "The `GridSearchCV.fit` method would fit your model to your data using each of those values separately. The results are then stored in your fitted object under `.cv_results_`. See the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html?highlight=gridsearchcv#sklearn.model_selection.GridSearchCV) for another example.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary example\n",
    "my_dict = {'chicken': 4}\n",
    "print(my_dict['chicken'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example use of GridSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "my_param_dict = {'l1_ratio':[0,0.1,0.4,0.8,1]}\n",
    "# pick a model\n",
    "# mdl = SomeModel()\n",
    "\n",
    "# gsv = GridSearchCV(mdl, param_grid=my_param_dict)\n",
    "# [something with gsv]\n",
    "# print(gsv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Data:\n",
    "x = (np.random.rand(500,1)-0.5)*5\n",
    "y = (x-3)*(x-0.1)*(x+2)*(4*x+4) + 15*np.random.randn(*x.shape)\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.6)\n",
    "\n",
    "hyperparam_search = {'l1_ratio': np.linspace(0,1,10), 'alpha': [1e-3, 1e-3, 1e-2, 0.1, 1]}\n",
    "\n",
    "mdl = ElasticNet(fit_intercept=False)\n",
    "poly = PolynomialFeatures(degree=4)  #### Change this value\n",
    "x_feat_train = poly.fit_transform(x_train)\n",
    "\n",
    "gsv = GridSearchCV(mdl, param_grid=hyperparam_search)\n",
    "gsv.fit(x_feat_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot our data:\n",
    "plt.figure()\n",
    "plt.plot(x,y,'.')\n",
    "x_curve = np.linspace(np.min(x), np.max(x), np.shape(x)[0])\n",
    "y_curve = (x_curve-3)*(x_curve-0.1)*(x_curve+2)*(4*x_curve+4)\n",
    "plt.plot(x_curve, y_curve)\n",
    "plt.legend(['Data','Generating Curve'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best split:\n",
    "#'rank_test_score' returns the ranking indices for each parameter set. The minimum ('1'), means the best set.\n",
    "best_ind = np.argmin(gsv.cv_results_['rank_test_score'])\n",
    "print(best_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = gsv.cv_results_['param_alpha'][best_ind]\n",
    "l1_ratio = gsv.cv_results_['param_l1_ratio'][best_ind]\n",
    "score = gsv.cv_results_['mean_test_score'][best_ind]\n",
    "\n",
    "print(f'alpha: {alpha}')\n",
    "print(f'l1_ratio: {l1_ratio}')\n",
    "print(f'score: {score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal values for `alpha` and `l1_ratio` are displayed above. These values will change depending on the degree of the fitted polynomial, and the available data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
