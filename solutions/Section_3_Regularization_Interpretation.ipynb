{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3\n",
    "## Regularization\n",
    "\n",
    "We previously looked at the effect of overfitting, and how we can detect it using cross-validation. Detecting overfitting is good, but what about preventing it from happening? Overfitting occurs when we have too many parameters relative to the amount of data we have. How can we modulate our model parameters such that overfitting is limited?  \n",
    "  \n",
    "The answer is _regularization_. Regularization is a penalty applied directly to model parameters _independent_ of the model performance. Without regularization, models are optimized relative to a particular metric. For example, the squared error:  \n",
    "\\begin{equation}\n",
    "    Err(Y, \\hat{Y}) = \\sum |Y-\\hat{Y}|^2\n",
    "\\end{equation}\n",
    "\n",
    "The metric relies only on predictions and labels. We can add a second term that applies only on the model parameters, `w`:  \n",
    "\n",
    "\\begin{equation}\n",
    "    Err(Y, \\hat{Y}) = \\sum (Y-\\hat{Y})^2 + \\alpha w^T w  \\\\\n",
    "    = \\sum |Y-\\hat{Y}|^2 + \\alpha \\sum w_i^2  \\\\\n",
    "    = (Prediction Loss) + (Regularization Loss)\n",
    "\\end{equation}\n",
    "\n",
    "The second term is the regularization term: the it increases with the square of the model parameters. Intuitively, this means that large model parameters are discouraged, while a mix of smaller parameters is preferred. This form of regularization is 'L2 regularization.' The weighting coefficient, $\\alpha$, determines the balance between regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Example 1 - L2\n",
    "Let's look at the effect of regularization on an increasing model complexity. In the first section, we saw how increasing model complexity via `PolynomialFeatures` resulted in overfitting of our model. Let's examine the effect of increasing the degree on `PolynomialFeatures` while using regularization.  \n",
    "In this example, we'll generate a noisy parabola, then try to fit polynomials of increasing complexity. In `sklearn`, regularization is often enabled via the `penalty` argument. In the case of linear regression, it is implemented in a separate class. For L2 regularization, we use `Ridge`, in the same way we previously used `LinearRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "x_data = np.random.random((200,1))\n",
    "y_label = (x_data-0.2)*(x_data-0.6) + np.random.randn(x_data.shape[0], 1)*0.05\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_label, train_size=0.4)\n",
    "plt.plot(x_data, y_label,'.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "# Create polynomials of features\n",
    "###\n",
    "poly_feat = PolynomialFeatures(degree=4)\n",
    "feat_train = poly_feat.fit_transform(x_train)\n",
    "###\n",
    "\n",
    "# Fit model to the new features\n",
    "\n",
    "# Create polynomials of features for test set\n",
    "# Evaluate on test set\n",
    "feat_test = poly_feat.fit_transform(x_test)\n",
    "\n",
    "linear_test_scr_list = []\n",
    "linear_train_scr_list = []\n",
    "linear_coef_mag_list = []  # Let's look at the regularization error\n",
    "\n",
    "ridge_test_scr_list = []\n",
    "ridge_train_scr_list = []\n",
    "ridge_coef_mag_list = []\n",
    "\n",
    "for i in range(20):  ############ You can modify this value here\n",
    "    poly_feat = PolynomialFeatures(degree=i)\n",
    "    feat_train = poly_feat.fit_transform(x_train)\n",
    "    \n",
    "    # Linear model\n",
    "    linear_mdl = LinearRegression(fit_intercept=False)\n",
    "    linear_mdl.fit(feat_train, y_train)\n",
    "    \n",
    "    # Ridge\n",
    "    ########## Modify the alpha parameter to modulate the relative importance of the prediction error\n",
    "    ########## vs. regularization\n",
    "    ridge_mdl = Ridge(fit_intercept=False, alpha=1)     \n",
    "    ridge_mdl.fit(feat_train, y_train)\n",
    "    \n",
    "    feat_test = poly_feat.fit_transform(x_test)\n",
    "    \n",
    "    linear_train_scr_list.append(linear_mdl.score(feat_train, y_train))\n",
    "    linear_test_scr_list.append(linear_mdl.score(feat_test, y_test))\n",
    "    linear_coef_mag_list.append(linear_mdl.coef_ @ linear_mdl.coef_.T)\n",
    "    \n",
    "    ridge_train_scr_list.append(ridge_mdl.score(feat_train, y_train))\n",
    "    ridge_test_scr_list.append(ridge_mdl.score(feat_test, y_test))\n",
    "    ridge_coef_mag_list.append(np.squeeze(ridge_mdl.coef_ @ ridge_mdl.coef_.T))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score plot\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(linear_train_scr_list, 'b.-')\n",
    "plt.plot(linear_test_scr_list, 'r.-')\n",
    "plt.xlabel('Degree fit')\n",
    "plt.ylabel('R2 Score')\n",
    "plt.legend(['Train Score', 'Test Score'])\n",
    "plt.title('Score vs. Degree (linear)')\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(ridge_train_scr_list, 'bx-')\n",
    "plt.plot(ridge_test_scr_list, 'r.-')\n",
    "plt.xlabel('Degree fit')\n",
    "plt.ylabel('R2 Score')\n",
    "plt.legend(['Train Score', 'Test Score'])\n",
    "plt.title('Score vs. Degree (reg.)')\n",
    "\n",
    "# Prediction plot\n",
    "plt.figure()\n",
    "plt.subplot(1,2,1)\n",
    "pred = linear_mdl.predict(feat_test)\n",
    "xind = np.argsort(x_test[:,0])\n",
    "plt.plot(x_test[xind], pred[xind],'r.-')\n",
    "plt.plot(x_test, y_test, 'b.')\n",
    "plt.xlabel('feature value')\n",
    "plt.ylabel('label value')\n",
    "plt.legend(['Labels','Prediction'])\n",
    "plt.title('Linear regression')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "pred = ridge_mdl.predict(feat_test)\n",
    "xind = np.argsort(x_test[:,0])\n",
    "plt.plot(x_test[xind], pred[xind],'r.-')\n",
    "plt.plot(x_test, y_test, 'b.')\n",
    "plt.xlabel('feature value')\n",
    "plt.ylabel('label value')\n",
    "plt.legend(['Labels','Prediction'])\n",
    "plt.title('Linear regression (reg.)')\n",
    "\n",
    "plt.figure()\n",
    "#plt.plot(linear_coef_mag_list, 'k.-')  # Uncomment this to show linear model coefficient magnitude\n",
    "plt.plot(ridge_coef_mag_list, 'b.-')\n",
    "#plt.legend(['Linear', 'Ridge'])\n",
    "plt.title('Model Param Magnitude vs. Complexity')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine the four plots and look at the differences between the non-regularized and regularized models. How does the performance differ with increasing model complexity? What about the predicted curve?  \n",
    "  \n",
    "Return to the Ridge initialization and modify the value of `alpha`, the weighting parameter for regularization. What is the effect of increasing the value? What is the effect of decreasing it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization Example 2 - L1  \n",
    "We saw the effect of L2 regularization in the previous example. Let's examine the effect of L1 regularization, implemented in `sklearn.linear_model.Lasso`. L1 regularization is implemented as:\n",
    "\\begin{equation}\n",
    "Err(Y, \\hat(Y)) = \\sum (Y-\\hat{Y})^2 + \\alpha \\sum |w|\n",
    "\\end{equation}\n",
    "The model parameters are penalized by the sum of the absolute value instead of the sum of squares. Intuitively, this results in a sparse representation; parameters that do not reduce the error proportional to their weight are removed.\n",
    "\n",
    "Instead of reproducing all of the previous plots, let's instead look at the parameter values directly. Using the data and `PolynomialFeatures` from above:  \n",
    "  \n",
    "1) Initialize and fit two models: Ridge and Lasso.  \n",
    "2) Print the model parameter magnitude (sum of squares of the coefficients) for degree 20.  \n",
    "3) Use a bar plot to compare parameter values across the polynomials. What difference do you see between the two?  \n",
    "  \n",
    "Note: There's a quirk with ridge.coef_; use np.squeeze(ridge.coef_) to get it into the expected shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import numpy as np\n",
    "\n",
    "# Create data\n",
    "x_data = np.random.random((500,1))\n",
    "y_label = (x_data-0.2)*(x_data-0.6) + np.random.randn(x_data.shape[0], 1)*0.01\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_label, train_size=0.4)\n",
    "\n",
    "# Create polynomials of features\n",
    "poly_feat = PolynomialFeatures(degree=20)\n",
    "feat_train = poly_feat.fit_transform(x_train)\n",
    "feat_test = poly_feat.fit_transform(x_test)\n",
    "\n",
    "# Initialize models\n",
    "ridge = Ridge(fit_intercept=False, alpha=0.1)  ### Vary alpha\n",
    "lasso = Lasso(fit_intercept=False, alpha=1e-6, max_iter=10000, tol=2e-3)  ### Vary alpha\n",
    "\n",
    "ridge.fit(feat_train, y_train)\n",
    "lasso.fit(feat_train, y_train)\n",
    "\n",
    "# Print magnitudes\n",
    "lasso_w = lasso.coef_\n",
    "print('L1 (Lasso) magnitudes: ' + str(lasso_w @ lasso_w.T))\n",
    "print('L1 score: ' + str(lasso.score(feat_test, y_test)))\n",
    "\n",
    "ridge_w = np.squeeze(ridge.coef_)\n",
    "print('L2 (Ridge) magnitude: ' + str(ridge_w @ ridge_w.T))\n",
    "print('L2 score: ' + str(ridge.score(feat_test, y_test)))\n",
    "\n",
    "# Bar plot\n",
    "plt.figure()\n",
    "plt.subplot(2,1,1)\n",
    "plt.bar(np.arange(len(lasso_w)), lasso_w)\n",
    "plt.title('L1 Regularized')\n",
    "plt.subplot(2,1,2)\n",
    "plt.bar(np.arange(len(ridge_w)), ridge_w)\n",
    "plt.title('L2 Regularized')\n",
    "plt.xlabel('Degree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the difference between the two distributions; what is the effect of L1 regularization on the parameter values? When would you prefer one over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "In the regularization examples, we saw the coefficient $\\alpha$ that determined how much our model parameters should be penalized relative to the prediction error. This type of parameter is not directly part of our model: it is not modified by our data, but it does directly affect our model's ability to generalize.  \n",
    "  \n",
    "In the example of logistic regression, we specified `solver`, which dictates which implementation will be used to determine the optimal parameters. Different implementations may not converge to a solution under certain conditions, which in turn would give us a poor model.  \n",
    "  \n",
    "External parameters that are not modified by data are called, 'hyperparameters'. Common ones include regularization weights, learning rate, or sample weights. The 'best' values for hyperparameters are determined via cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Interpretation\n",
    "\n",
    "Examine the model parameters in the last example, and compare them to the parameters for the generating function: $x^2 - 0.8x + 0.12$. Both models get in the general area of the parameter values: small positive intercept, negative weight for the linear term, and a positive weight for the quadratic term.  \n",
    "Neither of them is _quite_ right, though. They both have a cubic term, and the other terms aren't quite large enough.  \n",
    "  \n",
    "How would we interpret this model? Can we say that $x$ is twice as important as $x^2$ if its coefficient is twice as large?  \n",
    "  \n",
    "This is where machine learning and scientists might diverge: whereas machine learning may not care about what the model is so long as it performs well, scientists are interested in understanding what the model is doing; we are interested in _causality_. We want to know a feature's influence on the output. However, if we can't control our inputs, we can't really infer causality.  \n",
    "  \n",
    "We can make a slightly different statement: instead of, \"this feature causes X,\" we can say, \"this feature is useful for predicting X.\" It's the same distinction as correlation vs. causation.  \n",
    "  \n",
    "The distinction might be unpleasant, but we don't need to toss out the entirety of machine learning. We don't need one approach to solve every part of the problem. We can use our models to identify features of interest. We can have observational data, use ML models to identify useful features, then return to our experiments and perform causal experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "We saw in the regularization example that L1 regularization produces a _sparse_ representation; i.e., it implicitly reduces the number of non-zero coefficients. We might instead be interested in explicitly reducing the number of features, or simply seeing which features are most important for prediction performance.  \n",
    "  \n",
    "`sklearn` has a number of tools for feature selection ([link](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection)). We'll take a look at recursive feature elimination (RFE). It first fits your model to all of your input features, then fits all but of your input features, etc. The features which contribute the most to improving prediction are kept until the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection Exercise\n",
    "In this example, we'll examine the models that we saw in the regularization and see the performance of RFE.  \n",
    "  \n",
    "1) Generate data.  \n",
    "2) Augment the data using `PolynomialFeatures`  \n",
    "3) Initialize three models: LinearRegression, Lasso, Ridge  \n",
    "4) Use RFE to identify which features are important for each model. Print the rankings of each.  \n",
    "  \n",
    "Note: RFE stores rankings in rfe.ranking_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import RFE\n",
    "import numpy as np\n",
    "\n",
    "# Create data\n",
    "x_data = np.random.random((500,1))\n",
    "y_label = (x_data-0.2)*(x_data-0.6) + np.random.randn(x_data.shape[0], 1)*0.01\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_data, y_label, train_size=0.4)\n",
    "\n",
    "# Create polynomials of features\n",
    "poly_feat = PolynomialFeatures(degree=20)\n",
    "feat_train = poly_feat.fit_transform(x_train)\n",
    "feat_test = poly_feat.fit_transform(x_test)\n",
    "\n",
    "# Initialize models\n",
    "linear = LinearRegression()\n",
    "lasso = Lasso(fit_intercept=False, alpha=1e-6, max_iter=10000, tol=2e-3)  ### Vary alpha\n",
    "ridge = Ridge(fit_intercept=False, alpha=0.1)\n",
    "\n",
    "### Initialize selector; fit to each of linear, lasso, and ridge\n",
    "# help(RFE)\n",
    "selector = RFE(lasso, n_features_to_select=4)\n",
    "selector.fit(feat_train, y_train)\n",
    "print(selector.ranking_)\n",
    "\n",
    "selector = RFE(ridge, n_features_to_select=4)\n",
    "selector.fit(feat_train, y_train)\n",
    "print(selector.ranking_)\n",
    "\n",
    "selector = RFE(linear, n_features_to_select=4)\n",
    "selector.fit(feat_train, y_train)\n",
    "print(selector.ranking_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
